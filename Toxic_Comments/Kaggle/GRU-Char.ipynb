{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, SpatialDropout1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, AveragePooling1D, AveragePooling2D, GRU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "#df = pd.read_csv('./train.csv')\n",
    "#train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all blank comments with text in training set\n",
    "#extract training comments \n",
    "comments_train = train[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "#extract the toxciity ratings\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_ratings = train[classes].values\n",
    "#test_ratings = test[classes].values\n",
    "#extract test comments\n",
    "comments_test = test[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "\n",
    "#tokenizer the text\n",
    "#vectorize text\n",
    "tokenizer = text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(list(comments_train))\n",
    "tokenized_comments_train = tokenizer.texts_to_sequences(comments_train)\n",
    "tokenized_comments_test = tokenizer.texts_to_sequences(comments_test)\n",
    "#pad the text so each comment is uniform in length\n",
    "X_train = sequence.pad_sequences(tokenized_comments_train, maxlen=800, truncating='post')\n",
    "X_test = sequence.pad_sequences(tokenized_comments_test, maxlen=800,  truncating='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train2 = sequence.skipgrams(tokenized_comments_train, vocab_size, window_size=4, negative_samples=1.0, shuffle=False, categorical=False, sampling_table=None, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9178e0677c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/preprocessing/sequence.pyc\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 800, 800)          164800    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 800, 800)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 800, 256)          713472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 400, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 400, 128)          123264    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200, 64)           30912     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1500)              19201500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 9006      \n",
      "=================================================================\n",
      "Total params: 20,242,954\n",
      "Trainable params: 20,242,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define GRU sequential model\n",
    "gru = Sequential()\n",
    "gru.add(Embedding(vocab_size, output_dim=800, input_length=800))\n",
    "gru.add(SpatialDropout1D(.2))\n",
    "gru.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "gru.add(AveragePooling1D())\n",
    "gru.add(Bidirectional(GRU(64, return_sequences=True)))\n",
    "gru.add(AveragePooling1D())\n",
    "gru.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "gru.add(Flatten())\n",
    "gru.add(Dropout(.3))\n",
    "gru.add(Dense(1500, activation = 'relu'))\n",
    "gru.add(Dropout(.3))\n",
    "gru.add(Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint file\n",
    "file_path = \"gru_weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#early stop checkpoint\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "callbacks_list = [checkpoint, early] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.1046 - acc: 0.9689Epoch 00000: val_loss improved from inf to 0.08286, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1933s - loss: 0.1046 - acc: 0.9689 - val_loss: 0.0829 - val_acc: 0.9751\n",
      "Epoch 2/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0782 - acc: 0.9759Epoch 00001: val_loss improved from 0.08286 to 0.07825, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1884s - loss: 0.0781 - acc: 0.9759 - val_loss: 0.0783 - val_acc: 0.9753\n",
      "Epoch 3/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0704 - acc: 0.9774Epoch 00002: val_loss improved from 0.07825 to 0.06658, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1887s - loss: 0.0704 - acc: 0.9774 - val_loss: 0.0666 - val_acc: 0.9783\n",
      "Epoch 4/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0575 - acc: 0.9805Epoch 00003: val_loss improved from 0.06658 to 0.05890, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1887s - loss: 0.0575 - acc: 0.9805 - val_loss: 0.0589 - val_acc: 0.9797\n",
      "Epoch 5/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0501 - acc: 0.9824Epoch 00004: val_loss improved from 0.05890 to 0.05478, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1870s - loss: 0.0501 - acc: 0.9824 - val_loss: 0.0548 - val_acc: 0.9812\n",
      "Epoch 6/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0442 - acc: 0.9841Epoch 00005: val_loss improved from 0.05478 to 0.05451, saving model to gru_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1869s - loss: 0.0442 - acc: 0.9841 - val_loss: 0.0545 - val_acc: 0.9814\n",
      "Epoch 7/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0385 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 1871s - loss: 0.0385 - acc: 0.9859 - val_loss: 0.0559 - val_acc: 0.9812\n",
      "Epoch 8/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0328 - acc: 0.9879Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 1871s - loss: 0.0328 - acc: 0.9879 - val_loss: 0.0623 - val_acc: 0.9815\n",
      "Epoch 9/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0291 - acc: 0.9892Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 1871s - loss: 0.0291 - acc: 0.9892 - val_loss: 0.0624 - val_acc: 0.9812\n",
      "Epoch 10/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0244 - acc: 0.9909Epoch 00009: val_loss did not improve\n",
      "143613/143613 [==============================] - 1870s - loss: 0.0244 - acc: 0.9909 - val_loss: 0.0753 - val_acc: 0.9809\n",
      "Epoch 11/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0212 - acc: 0.9921Epoch 00010: val_loss did not improve\n",
      "143613/143613 [==============================] - 1869s - loss: 0.0212 - acc: 0.9921 - val_loss: 0.0798 - val_acc: 0.9815\n",
      "Epoch 12/15\n",
      "143500/143613 [============================>.] - ETA: 1s - loss: 0.0184 - acc: 0.9932Epoch 00011: val_loss did not improve\n",
      "143613/143613 [==============================] - 1869s - loss: 0.0184 - acc: 0.9932 - val_loss: 0.0705 - val_acc: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f12e0f11990>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.fit(X_train, train_ratings, batch_size=350, epochs=15, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best weights\n",
    "gru.load_weights(file_path)\n",
    "\n",
    "#make predictions on test set\n",
    "grupred = gru.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def calc_auc(y_true, y_pred):\n",
    "    return np.mean([roc_auc_score(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gru aucroc ', 0.90867178569693507)\n"
     ]
    }
   ],
   "source": [
    "auc = calc_auc(test_ratings, grupred)\n",
    "print('gru aucroc ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm =pd.read_csv('./sample_submission.csv')\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(grupred, columns = classes)], axis=1)\n",
    "submission.to_csv('GRU_char_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbsvm_sub = pd.read_csv('./NBSVM_char_submission.csv')\n",
    "gru_sub = pd.read_csv('./GRU_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "ensemble = gru_sub.copy()\n",
    "ensemble[labels] = (gru_sub[labels] + nbsvm_sub[labels]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm =pd.read_csv('./sample_submission.csv')\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(ensemble, columns = classes)], axis=1)\n",
    "submission.to_csv('GRU_NBSVM_char_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
