{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Toxic Comments__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chris Barcelon\n",
    "<br>January 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments that are toxic, such as threats, obscenity, insults, and identity-based hate,  are everywhere on the Internet.  The large number of toxic comments have prevented people from participating in discussions they are interested in and they have led to communities either limiting or restricting comments altogether.  A model that is capable of detecting these types of comments could hopefully help online discussions become more productive and respectful.\n",
    "In the paper Ex Machina: Personal Attacks Seen at Scale the authors discuss solving a similar problem initially using logistic regression and multi-layer perceptrons and planned to in the future use LSTM models.  This project is based off the kaggle competition __[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to build a model that can detect and classify different types of toxicity in comments.  Given a list of comments the model will predict the probability that each comment is toxic.  It will predict 6 different probabilities one for each 6 different types of toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be scored with a column-wise __[log loss](http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/)__ function.  The score will the be the average of the log loss of each predicted column, where each column is a different type of toxicity.  Log Loss will be used to socre the model because it quantifies the accuracy of a classifier by penalising false classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a csv file, train.csv, with a large number of Wikipeida comments that have been labeled by humans for toxicity.  Each comment is labled for 6 different types of toxicity: toxic, severe_toxic, obscene, threat, insult, identity_hate.\n",
    "train.csv contains 95850 comments each comment has an id number and a binary classification for each of the types of toxicity.\n",
    "\n",
    "<img src=\"first_10_comments.png\">\n",
    "\n",
    "Of the 95850 comments 9,789 are labeled for at least 1 type of toxicity, meaning the other 86061 comments are not toxic. The number of each comments labeled for each type of toxicity are as follows. \n",
    "- toxic - 9237\n",
    "- severe_toxic: 965\n",
    "- obscene: 5109\n",
    "- threat: 305\n",
    "- insult: 4765\n",
    "- identity_hate: 814\n",
    "<img src=\"Toxic_Comment_Counts.png\">\n",
    "\n",
    "The shortest comment in the dataset is only 1 word and the longest comment is 1403 word.  the average length for a comment is 67 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBSVM(Naive Bayes - Support Vector Machine) is a technique that is often used for text categorization and sentiment analysis.  In the paper __[Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)__ authors Sida Wang and Christopher D. Manning discuss the use of NBSVM in sentiment classification and topical text classification.  A NBSVM is a model which uses a linear approach on top of naive bayes features.  This model will take use a bag of words approach where each unique word is a feature in the Naive Bayes formula.  This will disregard word order and just look at whether a word appears in each comment and then calculate the probablity that a comment is toxic or not based on words that are in the comment.  The Naive Bayes equation used to calculate the probability is a follows: the **log-count ratio** $r$ for each word $f$ \n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in non-toxic comments}}{\\text{ratio of feature $f$ in toxic comments}}$\n",
    "\n",
    "where the ratio of feature $f$ in non-toxic comments is the number of times a non-toxic comment has a feature divided by the number of non-toxic comments.\n",
    "This aproach will only give the probability for one type of toxicity at a time so it will be run 6 times one time for each type of toxicity to give a probability for each type of toxicity for each comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory networks (LSTM) are a type of RNN (Recurrent Neural Network).  A RNN is a essentially a neural network on a loop, it can be thought of as multiple copies of the same network where each copy of the network passes some information to the next copy of the network.  This allows the netowrk to preserve data, this preserved data can then be used to help give context to a future data input.  A LSTM is a RNN that is especially good at preserving information over long periods of time.  Christopher Olah used the following diagram to help explain how LSTMs work.\n",
    "\n",
    "<img src=\"LSTM3-chain.png\">\n",
    "\n",
    "**The repeating module in an LSTM contains four interacting layers**\n",
    "\n",
    "<img src=\"LSTM2-notation.png\">\n",
    "\n",
    "In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.\n",
    "\n",
    "To gain a deeper understanding of LSTM networks I recomend reading the full post by Chrisopther Olah __[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)__\n",
    "\n",
    "While the NBSVM did not care about the word order and used a bag of words for the text, the LSTM network does care about the word order so it will require the text to be vectorized.  Also the LSTM network will be able to calculate the probablilties for each type of toxicity simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark model used is a basic bidirectional LSTM network provided by kaggle user CVxTz.  The kernel used can be found here: __https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051__\n",
    "A summary of the model used is provided."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_2 (InputLayer)         (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "embedding_2 (Embedding)      (None, 100, 128)          2560000   \n",
    "_________________________________________________________________\n",
    "bidirectional_2 (Bidirection (None, 100, 100)          71600     \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 100)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 50)                5050      \n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 50)                0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 6)                 306       \n",
    "=================================================================\n",
    "Total params: 2,636,956\n",
    "Trainable params: 2,636,956\n",
    "Non-trainable params: 0\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this model was evaluated after 1 epoch, using the predefined log los function it produced a socre of 0.0749. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any other preprocessing the data was split into 2 sets a training set and a testing set, the testing set is a random 20% of the initial dataset, the training set is the remaining 80% of the data.  The data was split using the sklearn.cross_validation.train_test_split function. \n",
    "\n",
    "To prepare the data for the LSTM model, the text of the comments needed to be converted to sequences to be trained on.  The first thing done to the text was to search through the comments and replace any empty comments with filler text in this case \"cbarcelon\" The text was then converted to sequences using the keras.preprocessing.text.Tokenizer and then to make the training more efficient the sequences were set to a uniform length using the pad_sequences function.  The initial length chosen is 100, 100 was chosen because is above the average length of 67 yet not too long.  It is believed that truncating long comments will not adversely effect the accuracy of the model because most comments will reveal their toxicity within the first 100 words.  To verify this belief different sequence lengths will be tested.\n",
    "\n",
    "To prepare the data for the NBSVM model a bag of words was created.  The text was tokenized and split into words and then was transformed into a matrix of word values.  A sparse matrix was used because most comments only use a small subset of the the total unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
