{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, AveragePooling1D, AveragePooling2D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the input dataset\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "#split the data into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Replace all blank comments with text in training set\n",
    "#extract training comments \n",
    "comments_train = train[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "#extract the toxciity ratings\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_ratings = train[classes].values\n",
    "test_ratings = test[classes].values\n",
    "#extract test comments\n",
    "comments_test = test[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "\n",
    "#tokenizer the text\n",
    "#vectorize text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(comments_train))\n",
    "tokenized_comments_train = tokenizer.texts_to_sequences(comments_train)\n",
    "tokenized_comments_test = tokenizer.texts_to_sequences(comments_test)\n",
    "#pad the text so each comment is uniform in length\n",
    "X_train = sequence.pad_sequences(tokenized_comments_train, maxlen=100, truncating='post')\n",
    "X_test = sequence.pad_sequences(tokenized_comments_test, maxlen=100,  truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 100, 512)          68773376  \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 150)          795600    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_19 (Averag (None, 50, 150)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 50, 150)           361200    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_20 (Averag (None, 25, 150)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 25, 150)           361200    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3750)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3750)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               1875500   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 3006      \n",
      "=================================================================\n",
      "Total params: 72,169,882\n",
      "Trainable params: 72,169,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define LSTM sequential model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(134323, output_dim=512, input_length=100))\n",
    "lstm.add(Bidirectional(LSTM(150, return_sequences=True), merge_mode='sum'))\n",
    "lstm.add(AveragePooling1D())\n",
    "lstm.add(Bidirectional(LSTM(150, return_sequences=True), merge_mode='sum'))\n",
    "lstm.add(AveragePooling1D())\n",
    "lstm.add(Bidirectional(LSTM(150, return_sequences=True), merge_mode='sum'))\n",
    "#lstm.add(AveragePooling1D())\n",
    "lstm.add(Flatten())\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(500, activation='relu'))\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint file\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#early stop checkpoint\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "callbacks_list = [checkpoint, early] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69012 samples, validate on 7668 samples\n",
      "Epoch 1/4\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9775Epoch 00000: val_loss improved from inf to 0.05347, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 774s - loss: 0.0697 - acc: 0.9775 - val_loss: 0.0535 - val_acc: 0.9811\n",
      "Epoch 2/4\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9836Epoch 00001: val_loss did not improve\n",
      "69012/69012 [==============================] - 729s - loss: 0.0433 - acc: 0.9836 - val_loss: 0.0541 - val_acc: 0.9807\n",
      "Epoch 3/4\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9868Epoch 00002: val_loss did not improve\n",
      "69012/69012 [==============================] - 728s - loss: 0.0332 - acc: 0.9868 - val_loss: 0.0593 - val_acc: 0.9800\n",
      "Epoch 4/4\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9897Epoch 00003: val_loss did not improve\n",
      "69012/69012 [==============================] - 728s - loss: 0.0258 - acc: 0.9897 - val_loss: 0.0737 - val_acc: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fe8a4bf60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train/fit the model\n",
    "lstm.fit(X_train, train_ratings, batch_size=50, epochs=4, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best weights\n",
    "lstm.load_weights(file_path)\n",
    "\n",
    "#make predictions on test set\n",
    "pred = lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log loss score function\n",
    "from sklearn.metrics import log_loss\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0520870097858\n"
     ]
    }
   ],
   "source": [
    "score = calc_loss(test_ratings, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  478   37 5892  663   10 4526 6989]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134323\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
