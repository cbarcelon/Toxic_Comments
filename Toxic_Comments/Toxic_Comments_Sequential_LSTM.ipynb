{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, AveragePooling1D, AveragePooling2D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the input dataset\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "#split the data into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Replace all blank comments with text in training set\n",
    "#extract training comments \n",
    "comments_train = train[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "#extract the toxciity ratings\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_ratings = train[classes].values\n",
    "test_ratings = test[classes].values\n",
    "#extract test comments\n",
    "comments_test = test[\"comment_text\"].fillna(\"cbarcelon\").values\n",
    "\n",
    "#tokenizer the text\n",
    "#vectorize text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(comments_train))\n",
    "tokenized_comments_train = tokenizer.texts_to_sequences(comments_train)\n",
    "tokenized_comments_test = tokenizer.texts_to_sequences(comments_test)\n",
    "#pad the text so each comment is uniform in length\n",
    "X_train = sequence.pad_sequences(tokenized_comments_train, maxlen=100, truncating='post')\n",
    "X_test = sequence.pad_sequences(tokenized_comments_test, maxlen=100,  truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_52 (Bidirectio (None, 100, 75)           122400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_50 (Averag (None, 50, 75)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_53 (Bidirectio (None, 50, 50)            50400     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_51 (Averag (None, 25, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_54 (Bidirectio (None, 25, 25)            15200     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_52 (Averag (None, 12, 25)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 1206      \n",
      "=================================================================\n",
      "Total params: 2,809,406\n",
      "Trainable params: 2,809,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define LSTM sequential model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(20000, output_dim=128, input_length=100))\n",
    "lstm.add(Bidirectional(LSTM(75, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(AveragePooling1D())\n",
    "lstm.add(Bidirectional(LSTM(50, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(AveragePooling1D())\n",
    "lstm.add(Bidirectional(LSTM(25, return_sequences=True), merge_mode='ave'))\n",
    "lstm.add(AveragePooling1D())\n",
    "lstm.add(Flatten())\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(200, activation='relu'))\n",
    "lstm.add(Dropout(.5))\n",
    "lstm.add(Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#create checkpoint file\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#early stop checkpoint\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks_list = [checkpoint, early] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69012 samples, validate on 7668 samples\n",
      "Epoch 1/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9596Epoch 00000: val_loss improved from inf to 0.11756, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 65s - loss: 0.1985 - acc: 0.9596 - val_loss: 0.1176 - val_acc: 0.9652\n",
      "Epoch 2/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9679Epoch 00001: val_loss improved from 0.11756 to 0.07032, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 62s - loss: 0.0961 - acc: 0.9679 - val_loss: 0.0703 - val_acc: 0.9761\n",
      "Epoch 3/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9781Epoch 00002: val_loss improved from 0.07032 to 0.05813, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 62s - loss: 0.0639 - acc: 0.9781 - val_loss: 0.0581 - val_acc: 0.9807\n",
      "Epoch 4/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9811Epoch 00003: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0532 - acc: 0.9811 - val_loss: 0.0606 - val_acc: 0.9786\n",
      "Epoch 5/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9829Epoch 00004: val_loss improved from 0.05813 to 0.05735, saving model to weights_base.best.hdf5\n",
      "69012/69012 [==============================] - 62s - loss: 0.0466 - acc: 0.9829 - val_loss: 0.0574 - val_acc: 0.9792\n",
      "Epoch 6/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9842Epoch 00005: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0421 - acc: 0.9842 - val_loss: 0.0588 - val_acc: 0.9790\n",
      "Epoch 7/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9855Epoch 00006: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0386 - acc: 0.9855 - val_loss: 0.0593 - val_acc: 0.9799\n",
      "Epoch 8/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0353 - acc: 0.9867 - val_loss: 0.1811 - val_acc: 0.9234\n",
      "Epoch 9/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9840Epoch 00008: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0438 - acc: 0.9840 - val_loss: 0.0636 - val_acc: 0.9790\n",
      "Epoch 10/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9874Epoch 00009: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0338 - acc: 0.9874 - val_loss: 0.0676 - val_acc: 0.9802\n",
      "Epoch 11/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9885Epoch 00010: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0311 - acc: 0.9885 - val_loss: 0.0693 - val_acc: 0.9799\n",
      "Epoch 12/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9894Epoch 00011: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0288 - acc: 0.9894 - val_loss: 0.0691 - val_acc: 0.9783\n",
      "Epoch 13/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9899Epoch 00012: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0274 - acc: 0.9899 - val_loss: 0.0717 - val_acc: 0.9777\n",
      "Epoch 14/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9902Epoch 00013: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0265 - acc: 0.9902 - val_loss: 0.0751 - val_acc: 0.9777\n",
      "Epoch 15/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9906Epoch 00014: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0257 - acc: 0.9906 - val_loss: 0.0736 - val_acc: 0.9774\n",
      "Epoch 16/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9904Epoch 00015: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0258 - acc: 0.9904 - val_loss: 0.1202 - val_acc: 0.9542\n",
      "Epoch 17/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9887Epoch 00016: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0299 - acc: 0.9887 - val_loss: 0.0757 - val_acc: 0.9779\n",
      "Epoch 18/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9910Epoch 00017: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0241 - acc: 0.9910 - val_loss: 0.0780 - val_acc: 0.9792\n",
      "Epoch 19/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9919Epoch 00018: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0220 - acc: 0.9919 - val_loss: 0.0802 - val_acc: 0.9781\n",
      "Epoch 20/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9928Epoch 00019: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0199 - acc: 0.9928 - val_loss: 0.0986 - val_acc: 0.9794\n",
      "Epoch 21/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9921Epoch 00020: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0214 - acc: 0.9921 - val_loss: 0.0858 - val_acc: 0.9774\n",
      "Epoch 22/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9931Epoch 00021: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0192 - acc: 0.9931 - val_loss: 0.0853 - val_acc: 0.9787\n",
      "Epoch 23/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9935Epoch 00022: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0182 - acc: 0.9935 - val_loss: 0.0899 - val_acc: 0.9790\n",
      "Epoch 24/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9940Epoch 00023: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0169 - acc: 0.9940 - val_loss: 0.0956 - val_acc: 0.9775\n",
      "Epoch 25/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9936Epoch 00024: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0175 - acc: 0.9936 - val_loss: 0.0907 - val_acc: 0.9770\n",
      "Epoch 26/200\n",
      "69000/69012 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9937Epoch 00025: val_loss did not improve\n",
      "69012/69012 [==============================] - 62s - loss: 0.0173 - acc: 0.9937 - val_loss: 0.0933 - val_acc: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f204482ff98>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train/fit the model\n",
    "lstm.fit(X_train, train_ratings, batch_size=750, epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best weights\n",
    "lstm.load_weights(file_path)\n",
    "\n",
    "#make predictions on test set\n",
    "pred = lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log loss score function\n",
    "from sklearn.metrics import log_loss\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0621659652706\n"
     ]
    }
   ],
   "source": [
    "score = calc_loss(test_ratings, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
