{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Toxic Comments__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chris Barcelon\n",
    "<br>January 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments that are toxic, such as threats, obscenity, insults, and identity-based hate,  are everywhere on the Internet.  The large number of toxic comments have prevented people from participating in discussions they are interested in and they have led to communities either limiting or restricting comments altogether.  A model that is capable of detecting these types of comments could hopefully help online discussions become more productive and respectful.\n",
    "In the paper Ex Machina: Personal Attacks Seen at Scale the authors discuss solving a similar problem initially using logistic regression and multi-layer perceptrons and planned to in the future use LSTM models.  This project is based off the kaggle competition __[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to build a model that can detect and classify different types of toxicity in comments.  Given a list of comments the model will predict the probability that each comment is toxic.  It will predict 6 different probabilities one for each 6 different types of toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be scored with a column-wise __[log loss](http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/)__ function.  The score will the be the average of the log loss of each predicted column, where each column is a different type of toxicity.  Log Loss will be used to socre the model because it quantifies the accuracy of a classifier by penalising false classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a csv file, train.csv, with a large number of Wikipeida comments that have been labeled by humans for toxicity.  Each comment is labled for 6 different types of toxicity: toxic, severe_toxic, obscene, threat, insult, identity_hate.\n",
    "train.csv contains 95850 comments each comment has an id number and a binary classification for each of the types of toxicity.\n",
    "\n",
    "Of the 95850 comments 9,789 are labeled for at least 1 type of toxicity, meaning the other 86061 comments are not toxic. The number of each comments labeled for each type of toxicity are as follows. \n",
    "- toxic - 9237\n",
    "- severe_toxic: 965\n",
    "- obscene: 5109\n",
    "- threat: 305\n",
    "- insult: 4765\n",
    "- identity_hate: 814\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
